{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a420c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # Order GPUs by PCI bus number\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"         # Select GPU with PCI bus number 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52458c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  7 15:22:33 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    61W / 400W |   3872MiB / 40960MiB |      4%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   327W / 400W |  22942MiB / 40960MiB |     97%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    49W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3704116      C   python                           3869MiB |\n",
      "|    1   N/A  N/A   3753573      C   python                          22939MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38ede8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58d5c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaonanl5\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/haonan.li/scripts/wandb/run-20230307_152240-ey0g6f9a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/haonanl5/misc/runs/ey0g6f9a' target=\"_blank\">light-deluge-9</a></strong> to <a href='https://wandb.ai/haonanl5/misc' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/haonanl5/misc' target=\"_blank\">https://wandb.ai/haonanl5/misc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/haonanl5/misc/runs/ey0g6f9a' target=\"_blank\">https://wandb.ai/haonanl5/misc/runs/ey0g6f9a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/haonanl5/misc/runs/ey0g6f9a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f984de32530>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='misc', entity='haonanl5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad2d15",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e639d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/l/users/haonan.li/LAMB/data/TourQue_Knowledge_Sel.json') as f:\n",
    "    data = pd.read_json(f, orient='index')\n",
    "data = data.dropna() # remove the lines without latlong    \n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class PlacesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, latlongs):\n",
    "        self.encodings = encodings\n",
    "        self.latlong = latlongs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['latlong'] = torch.tensor(self.latlong[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latlong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878ef93",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a093f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "model.transformer.layer = model.transformer.layer[:n_layers] # keep only n_layers \n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc69cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer(train_data['name'].tolist(), padding=True, truncation=True, max_length=64)\n",
    "test_encodings = tokenizer(test_data['name'].tolist(), padding=True, truncation=True, max_length=64)\n",
    "\n",
    "# Create the PyTorch datasets and data loaders\n",
    "train_dataset = PlacesDataset(train_encodings, train_data['lat_long'].tolist())\n",
    "test_dataset = PlacesDataset(test_encodings, test_data['lat_long'].tolist())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df5ff7",
   "metadata": {},
   "source": [
    "### Training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568c1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "max_seq_len = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the optimizer and the learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfb15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PlaceDistanceLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(PlaceDistanceLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embeddings, coords, device):\n",
    "        # embeddings is the tensor of fixed-length representations of the place names\n",
    "        # coords is the tensor of geocoordinates\n",
    "        \n",
    "        # Compute pairwise distances between all pairs of places\n",
    "        # Convert latitude and longitude to radians\n",
    "        coordinates_rad = torch.deg2rad(coords)\n",
    "\n",
    "        # Compute pairwise differences in latitude and longitude\n",
    "        dlat = coordinates_rad[:, None, 0] - coordinates_rad[None, :, 0]\n",
    "        dlon = coordinates_rad[:, None, 1] - coordinates_rad[None, :, 1]\n",
    "\n",
    "        # Compute great-circle distance using Haversine formula\n",
    "        a = torch.sin(dlat / 2) ** 2 + torch.cos(coordinates_rad[:, 0])[:, None] * torch.cos(coordinates_rad[None, :, 0]) * torch.sin(dlon / 2) ** 2\n",
    "        c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "        distances = c / math.pi  # Earth radius = 6371 km\n",
    "\n",
    "        # distance is a tensor of shape (num_points, num_points) containing the true pairwise distances\n",
    "        \n",
    "        # Compute pairwise cosine similarities between all pairs of fixed-length representations\n",
    "        similarities = torch.nn.functional.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # Convert distances to similarities using a Gaussian kernel\n",
    "        sigma = distances.std()\n",
    "        similarities_gt = torch.exp(-distances ** 2 / (2 * sigma ** 2))\n",
    "        \n",
    "        # Compute the triplet loss\n",
    "        margin = self.margin\n",
    "        N = embeddings.size(0)\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                for k in range(j):\n",
    "                    if k == i or k == j:\n",
    "                        continue\n",
    "                    \n",
    "                    a_sim = similarities[i, j]\n",
    "                    b_sim = similarities[i, k]\n",
    "                    a_sim_gt = similarities_gt[i, j]\n",
    "                    b_sim_gt = similarities_gt[i, k]\n",
    "                    \n",
    "                    flag = a_sim_gt - b_sim_gt\n",
    "                    # print(pos_sim, neg_sim, pos_sim_gt, neg_sim_gt)\n",
    "                    # Compute the triplet loss\n",
    "                    if flag < 0:\n",
    "                        triplet_loss = torch.max(a_sim - b_sim - flag, torch.tensor(0.0).to(device))\n",
    "                    else:\n",
    "                        triplet_loss = torch.max(b_sim - a_sim + flag, torch.tensor(0.0).to(device))\n",
    "                    # Add the triplet loss to the total loss\n",
    "                    loss += triplet_loss # + triplet_loss_gt\n",
    "        \n",
    "        # Normalize the loss by the number of triplets\n",
    "        num_triplets = N * (N - 1) * (N - 2) / 2\n",
    "        loss /= num_triplets\n",
    "        \n",
    "        return loss\n",
    "criterion = PlaceDistanceLoss(margin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445f5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        latlong = batch['latlong'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        rep = outputs.last_hidden_state[:,0,:]\n",
    "        loss = criterion(rep, latlong.float(), device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"Train Loss\":loss.item()})\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Define the evaluation loop\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            latlong = batch['latlong'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            rep = outputs.last_hidden_state[:,0,:]\n",
    "            loss = criterion(rep, latlong.float(), device)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96cb61cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/home/haonan.li/.conda/envs/torch13cu117/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    wandb.log({\"Avg Train Loss\":train_loss, \"Test Loss\": test_loss})\n",
    "    model.save_pretrained(f'/l/users/haonan.li/LAMB/data/tmp/loc_{n_layers}layer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662f9751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cb0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
